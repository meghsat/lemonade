# openvino_bench.py,
import argparse
import psutil
import statistics
from statistics import StatisticsError
from lemonade.state import State
from lemonade.cache import Keys
from lemonade.tools.bench import Bench
from lemonade.tools.adapter import ModelAdapter, TokenizerAdapter
import openvino_genai as ov_genai
import os
import json

default_iterations = 10
default_warmup_runs = 5
default_prompt_length = 64
default_output_tokens = 32
default_prompt = "Hello, I am conscious and"


class OpenVinoBench(Bench):
    """
    Benchmarking class for OpenVINO GenAI models.
    """

    unique_name = "openvino-bench"

    def __init__(self):
        super().__init__()

        # Additional statistics generated by this bench tool
        self.status_stats.insert(
            self.status_stats.index(Keys.TOKEN_GENERATION_TOKENS_PER_SECOND) + 1,
            Keys.STD_DEV_TOKENS_PER_SECOND,
        )
        self.std_dev_token_generation_tokens_per_second_list = []

        # Per-prompt power metrics lists (HWInfo)
        self.peak_processor_package_power_list = []
        self.avg_processor_package_power_list = []
        self.power_plot_list = []

        # Per-prompt label tracking
        self.prompt_labels = []

    @staticmethod
    def parser(parser: argparse.ArgumentParser = None, add_help: bool = True):
        # Allow inherited classes to initialize and pass in a parser, add parameters to it if so
        if parser is None:
            parser = __class__.helpful_parser(
                short_description="Benchmark on OpenVINO",
                add_help=add_help,
            )

        parser = Bench.parser(parser)

        return parser

    def get_prompt_str(self, state, token_length):
        """
        Returns a string with the prescribed token length.
        """
        self.input_ids_len_list.append(token_length)
        return "word " * (token_length - 1)

    def run(
        self,
        state: State,
        prompts: list[str] = None,
        iterations: int = default_iterations,
        warmup_iterations: int = default_warmup_runs,
        output_tokens: int = default_output_tokens,
        **kwargs,
    ) -> State:
        """
        Args:
            - prompts: List of input prompts used as starting points for LLM text generation
            - iterations: number of benchmarking samples to take; results are
                reported as the median and mean of the samples.
            - warmup_iterations: subset of the iterations to treat as warmup,
                and not included in the results.
            - output_tokens: Number of new tokens LLM to create.
            - kwargs: Additional parameters used by bench tools
        """
        import time
        import lemonade.common.printing as printing
        from datetime import datetime

        if prompts is None:
            prompts = ["word " * (default_prompt_length - 2)]
        elif isinstance(prompts, str):
            prompts = [prompts]

        state.save_stat("prompts", prompts)
        state.save_stat("iterations", iterations)
        state.save_stat("warmup_iterations", warmup_iterations)
        state.save_stat("output_tokens", output_tokens)

        # Check if profilers are available
        has_profilers = hasattr(state, 'profilers') and state.profilers

        counter = 0
        report_progress_fn = lambda x: self.set_percent_progress(
            100 * (counter + x) / len(prompts)
        )
        self.first_run_prompt = True
        for counter, prompt in enumerate(prompts):
            report_progress_fn(0)

            # Get prompt label for this prompt
            prompt_label = f"Prompt {counter + 1}: {prompt[:50]}..." if len(str(prompt)) > 50 else f"Prompt {counter + 1}: {prompt}"
            self.prompt_labels.append(prompt_label)

            # Store current prompt label in state for profiler to access
            state.current_prompt_label = prompt_label

            # Restart profilers for this prompt if we're not on the first prompt
            if has_profilers and counter > 0:
                # Stop profilers from previous prompt
                for profiler in state.profilers:
                    profiler.stop()

                # Wait for cooldown
                gpu_cooldown = kwargs.get('gpu_cooldown', 5)
                printing.log_info(f"GPU cooldown for {gpu_cooldown}s before next prompt...")
                time.sleep(gpu_cooldown)

                # Restart profilers for this prompt
                build_dir = state.cache_dir + "/builds/" + state.build_name
                for profiler in state.profilers:
                    profiler.start(build_dir)

                # Update start times for this prompt
                state.profiler_start_times[f"prompt_{counter}"] = time.time()

            self.run_prompt(
                state,
                report_progress_fn,
                prompt,
                iterations,
                warmup_iterations,
                output_tokens,
                **kwargs,
            )
            self.first_run_prompt = False

            if self.save_max_memory_used:
                self.max_memory_used_gb_list.append(
                    psutil.Process().memory_info().peak_wset / 1024**3
                )

            # Generate profiler results for this prompt
            if has_profilers:
                for profiler in state.profilers:
                    profiler.stop()

                # Generate plot with prompt-specific timestamp
                current_time = datetime.now()
                prompt_timestamp = current_time.strftime(f"%Y-%m-%d-%H%M%S_prompt_{counter}")

                for profiler in state.profilers:
                    profiler.generate_results(state, prompt_timestamp, state.profiler_start_times)

                # Store per-prompt power metrics
                self._store_power_metrics(state)

            # Display results for this prompt immediately (after storing metrics)
            self.display_prompt_results(state, prompt, counter)

        # Mark that we've already handled profiler output per-prompt
        if has_profilers:
            state.profilers_handled_by_tool = True

        self.set_percent_progress(None)
        self.save_stats(state)

        return state

    def run_prompt(
        self,
        state: State,
        report_progress_fn,
        prompt: str,
        iterations: int,
        warmup_iterations: int,
        output_tokens: int,
        **kwargs,
    ) -> State:
        ov_prompt = [prompt]
        model: ModelAdapter = state.model
        tokenizer: TokenizerAdapter = state.tokenizer

        per_iteration_time_to_first_token = []
        per_iteration_tokens_per_second = []

        # setting state variables to be used for hwinfo graphing and power measurements
        state.bench_intervals = {}
        state.bench_num_intervals = iterations
        state.bench_segements = ["prefill", "generation", "cooldown"]
        state.bench_output_tokens = output_tokens

        # Warm up iterations
        for count in range(warmup_iterations):
            print(f"Warmup Iteration {count}")
            result, start_time, end_time = model.generate(
                ov_prompt, min_new_tokens=output_tokens, max_new_tokens=output_tokens
            )
            ov_perf_metrics = result.perf_metrics

            report_progress_fn((count + 1) / (warmup_iterations + iterations))

        # Execute iterations and record performance metrics and time spent per interval
        for count in range(iterations):
            result, start_time, end_time = model.generate(
                ov_prompt, min_new_tokens=output_tokens, max_new_tokens=output_tokens
            )
            ov_perf_metrics = result.perf_metrics

            ttft_seconds = ov_perf_metrics.get_ttft().mean / 1000.0
            tokenization_duration = (
                ov_perf_metrics.get_tokenization_duration().mean / 1000.0
            )

            # setting start times of key intervals of this iteration
            state.bench_intervals[f"prefill {count}"] = (
                start_time + tokenization_duration
            )
            state.bench_intervals[f"generation {count}"] = (
                state.bench_intervals[f"prefill {count}"] + ttft_seconds
            )
            state.bench_intervals[f"cooldown {count}"] = end_time

            print(f"Iteration {count}")
            print(f"result: \n {result}")

            print(f"Load time: {ov_perf_metrics.get_load_time():.2f} ms")
            print(
                f"TPOT: {ov_perf_metrics.get_tpot().mean:.2f} ± {ov_perf_metrics.get_tpot().std:.2f} ms"
            )
            print(
                f"TTFT: {ov_perf_metrics.get_ttft().mean:.2f} ± {ov_perf_metrics.get_ttft().std:.2f} ms"
            )
            print(
                f"Throughput : {ov_perf_metrics.get_throughput().mean:.2f} ± {ov_perf_metrics.get_throughput().std:.2f} tokens/s"
            )

            print(f"Input Tokens: {ov_perf_metrics.get_num_input_tokens()}")
            print(f"Output Tokens: {ov_perf_metrics.get_num_generated_tokens()}")

            self.input_ids_len_list.append(ov_perf_metrics.get_num_input_tokens())
            self.tokens_out_len_list.append(ov_perf_metrics.get_num_generated_tokens())
            per_iteration_time_to_first_token.append(
                ov_perf_metrics.get_ttft().mean / 1000
            )  # TTFT is in milliseconds
            per_iteration_tokens_per_second.append(
                (1000 / ov_perf_metrics.get_tpot().mean)
            )  # TPOT is in milliseconds

            report_progress_fn(
                (warmup_iterations + count + 1) / (warmup_iterations + iterations)
            )

        mean_time_to_first_token = statistics.mean(per_iteration_time_to_first_token)
        self.mean_time_to_first_token_list.append(mean_time_to_first_token)
        self.prefill_tokens_per_second_list.append(
            self.input_ids_len_list[0] / mean_time_to_first_token
        )
        self.token_generation_tokens_per_second_list.append(
            statistics.mean(per_iteration_tokens_per_second)
        )
        try:
            self.std_dev_time_to_first_token_list.append(
                statistics.stdev(per_iteration_time_to_first_token)
            )
        except StatisticsError:
            # Less than 2 measurements
            self.std_dev_time_to_first_token_list.append(None)
        try:
            self.std_dev_token_generation_tokens_per_second_list.append(
                statistics.stdev(per_iteration_tokens_per_second)
            )
        except StatisticsError:
            # Less than 2 measurements
            self.std_dev_token_generation_tokens_per_second_list.append(None)

    def display_prompt_results(self, state, prompt, prompt_index):
        """Display results for a single prompt immediately after benchmarking"""
        import lemonade.common.printing as printing
        from lemonade.cache import Keys
        import sys
        import csv
        import os

        # Get the real terminal stdout (not redirected by Logger)
        # The Logger redirects sys.stdout, but saves the original as self.terminal
        output = sys.stdout
        if hasattr(sys.stdout, 'terminal'):
            output = sys.stdout.terminal

        # Get prompt label and actual token count
        if len(self.prompt_labels) > prompt_index:
            prompt_label = self.prompt_labels[prompt_index]
        else:
            prompt_label = f"{prompt}"

        # Get actual token count from benchmark results
        actual_token_count = self.input_ids_len_list[prompt_index] if len(self.input_ids_len_list) > prompt_index else "N/A"

        # Print newline and separator directly to terminal
        output.write(f"\n{'='*80}\n")
        output.write(f"Results for Prompt: {prompt_label} ({actual_token_count} tokens) (Prompt {prompt_index + 1})\n")
        output.write(f"{'='*80}\n")
        output.flush()

        # Get the current prompt's results (last item in each list)
        idx = prompt_index

        results = {
            "Prompt Tokens": self.input_ids_len_list[idx],
            "Response Tokens": self.tokens_out_len_list[idx],
            "Seconds To First Token": f"{self.mean_time_to_first_token_list[idx]:.3f}",
            "Token Generation Tokens Per Second": f"{self.token_generation_tokens_per_second_list[idx]:.3f}",
            "Prefill Tokens Per Second": f"{self.prefill_tokens_per_second_list[idx]:.3f}",
        }

        # Add std dev metrics if available
        if len(self.std_dev_time_to_first_token_list) > idx and self.std_dev_time_to_first_token_list[idx] is not None:
            results["Std Dev Seconds To First Token"] = f"{self.std_dev_time_to_first_token_list[idx]:.3f}"

        if len(self.std_dev_token_generation_tokens_per_second_list) > idx and self.std_dev_token_generation_tokens_per_second_list[idx] is not None:
            results["Std Dev Tokens Per Second"] = f"{self.std_dev_token_generation_tokens_per_second_list[idx]:.3f}"

        if self.save_max_memory_used and len(self.max_memory_used_gb_list) > idx:
            if self.max_memory_used_gb_list[idx] is not None:
                results["Memory Usage (GB)"] = f"{self.max_memory_used_gb_list[idx]:.3f}"

        # Display power metrics from per-prompt lists (HWInfo)
        if len(self.peak_processor_package_power_list) > idx:
            results["Peak Processor Package Power (HWInfo)"] = self.peak_processor_package_power_list[idx]
            results["Avg Processor Package Power (HWInfo)"] = self.avg_processor_package_power_list[idx]

            # Add plot path if available
            if len(self.power_plot_list) > idx:
                results["Power Usage Plot"] = self.power_plot_list[idx]

        for key, value in results.items():
            output.write(f"  {key}: {value}\n")

        output.write(f"{'='*80}\n\n")
        output.flush()

        # CSV export: Append results to CSV file
        self._append_to_csv(state, prompt_label, results)

    def _append_to_csv(self, state, prompt_label, results):
        """Append benchmark results to a CSV file in the cache directory"""
        import csv
        import os
        from datetime import datetime

        csv_filename = "benchmark_results.csv"
        csv_path = os.path.join(state.cache_dir, csv_filename)

        csv_row = {"Prompt": prompt_label}

        for key, value in results.items():
            if isinstance(value, str) and key != "Power Usage Plot":
                try:
                    csv_row[key] = float(value)
                except ValueError:
                    csv_row[key] = value
            else:
                csv_row[key] = value

        file_exists = os.path.exists(csv_path)

        fieldnames = ["Prompt"] + [k for k in results.keys()]

        try:
            with open(csv_path, 'a', newline='') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                if not file_exists:
                    writer.writeheader()

                writer.writerow(csv_row)

            if not file_exists:
                print(f"Created CSV file: {csv_path}")
            else:
                print(f"Appended results to: {csv_path}")

        except Exception as e:
            print(f"Warning: Failed to write to CSV: {e}")

    def _store_power_metrics(self, state):
        """Store per-prompt power metrics from filesystem into lists"""
        from lemonade.profilers.hwinfo_power import Keys as HWInfoKeys
        import lemonade.common.filesystem as fs

        # Load stats from the YAML file (not from state.stats attribute)
        stats_obj = fs.Stats(state.cache_dir, state.build_name)
        stats = stats_obj.stats  # This loads from lemonade_stats.yaml

        if stats:
            # Check for HWInfo power metrics
            if HWInfoKeys.PEAK_PROCESSOR_PACKAGE_POWER in stats:
                self.peak_processor_package_power_list.append(stats[HWInfoKeys.PEAK_PROCESSOR_PACKAGE_POWER])
                # Calculate average from power usage data if available
                if HWInfoKeys.POWER_USAGE_DATA in stats:
                    power_data = stats[HWInfoKeys.POWER_USAGE_DATA]
                    if power_data and len(power_data) > 0:
                        # Calculate weighted average power across all stages
                        total_energy = sum(stage.get('energy consumed', 0) for stage in power_data)
                        total_duration = sum(stage.get('duration', 0) for stage in power_data)
                        avg_power = total_energy / total_duration if total_duration > 0 else 0
                        self.avg_processor_package_power_list.append(f"{avg_power:.1f} W")
                    else:
                        self.avg_processor_package_power_list.append("N/A")
                else:
                    self.avg_processor_package_power_list.append("N/A")

                if HWInfoKeys.POWER_USAGE_PLOT in stats:
                    self.power_plot_list.append(stats[HWInfoKeys.POWER_USAGE_PLOT])

    def save_stats(self, state):
        super().save_stats(state)

        # Save additional statistics
        if not all(
            element is None
            for element in self.std_dev_token_generation_tokens_per_second_list
        ):
            state.save_stat(
                Keys.STD_DEV_TOKENS_PER_SECOND,
                self.get_item_or_list(
                    self.std_dev_token_generation_tokens_per_second_list
                ),
            )

        # Save power metrics lists if they have data
        from lemonade.profilers.hwinfo_power import Keys as HWInfoKeys

        if self.peak_processor_package_power_list:
            # Determine if it's single or list
            peak_power = self.peak_processor_package_power_list[0] if len(self.peak_processor_package_power_list) == 1 else self.peak_processor_package_power_list
            avg_power = self.avg_processor_package_power_list[0] if len(self.avg_processor_package_power_list) == 1 else self.avg_processor_package_power_list

            state.save_stat(HWInfoKeys.PEAK_PROCESSOR_PACKAGE_POWER, peak_power)
            state.save_stat("avg_processor_package_power_hwinfo", avg_power)

            if self.power_plot_list:
                plots = self.power_plot_list[0] if len(self.power_plot_list) == 1 else self.power_plot_list
                state.save_stat(HWInfoKeys.POWER_USAGE_PLOT, plots)
